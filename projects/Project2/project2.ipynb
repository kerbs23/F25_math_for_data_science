{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gdC70xxFyc4",
    "lines_to_next_cell": 2
   },
   "source": [
    "Math 5750/6880: Mathematics of Data Science \\\n",
    "Project 2\n",
    "pyright: basic\n",
    "ruff: noqa: e402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9_7SnpMGKDJ"
   },
   "source": [
    "# 1. Clustering Gaussian Blobs using $k$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AB136H0PGKq1",
    "outputId": "6fcea671-2dcb-4b43-c98a-83ec001de164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1000, 10)\n",
      "<class 'numpy.ndarray'> (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate 5 Gaussian blobs in 10 dimensions\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=1000,\n",
    "    centers=5,\n",
    "    n_features=10,\n",
    "    cluster_std=1.5,\n",
    "    random_state=1)        # reproducibility\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(type(X),X.shape)\n",
    "print(type(y_true),y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5GAsN-dmHjRM"
   },
   "outputs": [],
   "source": [
    "# Start with a basic k-means analysys of the data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data=X)\n",
    "df.columns = [f'X_{i+1}' for i in range(len(df.columns))]\n",
    "df['true_category'] = y_true\n",
    "\n",
    "kmeans_5 = KMeans(n_clusters=5).fit(X) # because this is using k-means++ sampling there is only one run, so lowest inertia is just the inertia of the output\n",
    "print(f'Lowest inertia: {kmeans_5.inertia_}')\n",
    "df['kmeans_5_pred_category'] = kmeans_5.labels_\n",
    "\n",
    "centers = kmeans_5.cluster_centers_\n",
    "\n",
    "# PCA down to 2 dim\n",
    "X_pca_2d = PCA(n_components=2).fit(X)\n",
    "df[['X_pca_2d_1', 'X_pca_2d_2']] = X_pca_2d.transform(X)\n",
    "centers_2d = X_pca_2d.transform(centers)\n",
    "\n",
    "print(df.head)\n",
    "df.to_csv(\"gausian_blob_processed.csv\")\n",
    "\n",
    "# Plot it\n",
    "plt.scatter(df['X_pca_2d_1'], df['X_pca_2d_2'], c=df['kmeans_5_pred_category'], cmap='viridis')\n",
    "plt.scatter(centers_2d[:, 0], centers_2d[:, 1], c='red', marker='X', s=200)\n",
    "plt.savefig('latex_jail/plots/KMeans_plot.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# now to re-index to match the predicted and real columns\n",
    "# going to just take the mode of each match, and if there are no collisions call it good\n",
    "\n",
    "# Find modal mapping\n",
    "modal_map = df.groupby('kmeans_5_pred_category')['true_category'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "\n",
    "# Check if mapping is one-to-one\n",
    "print(modal_map)\n",
    "\n",
    "# Apply transformation\n",
    "df['kmeans_5_pred_category_adj'] = df['kmeans_5_pred_category'].map(modal_map) # pyright: ignore[]\n",
    "\n",
    "# create the confusion matrix\n",
    "confusion_matrix_kmean5 = confusion_matrix(df['true_category'], df['kmeans_5_pred_category_adj'])\n",
    "print(confusion_matrix_kmean5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2qcKggmIH8T"
   },
   "source": [
    "\n",
    "# 2. Clustering Fashion-MNIST using $k$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9IQwhgcIVOl",
    "outputId": "5cc76846-93c1-492c-a1ab-6388f8300da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (70000, 784)\n",
      "<class 'numpy.ndarray'> (70000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Fashion-MNIST from OpenML\n",
    "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "X, y = fetch_openml(\"Fashion-MNIST\", version=1, as_frame=False, parser=\"auto\", return_X_y=True)\n",
    "y = y.astype(int)\n",
    "\n",
    "print(type(X),X.shape)\n",
    "print(type(y),y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbf120",
   "metadata": {
    "id": "0REsDBunNmEl"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "# think it would be cool to see the data through the process\n",
    "def peek_img(X=X, y=y, savepath=None):\n",
    "    # get one unique sample per class\n",
    "    unique_indices = []\n",
    "    for class_id in range(10):\n",
    "        idx = np.where(y == class_id)[0][0]\n",
    "        unique_indices.append(idx)\n",
    "    \n",
    "    # Create figure with labels\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    side_length = int(np.sqrt(X.shape[1]))\n",
    "\n",
    "\n",
    "    for i, idx in enumerate(unique_indices):\n",
    "        ax = axes.flat[i]\n",
    "        ax.imshow(X[idx].reshape(side_length,side_length), cmap='gray')\n",
    "        ax.set_title(class_names[y[idx]])\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "peek_img(savepath='latex_jail/plots/examble_items_fashion.png')\n",
    "\n",
    "# Scale the data\n",
    "X = StandardScaler().fit_transform(X)\n",
    "peek_img()\n",
    "\n",
    "# It does not appear to need a sample size reduction for the kmeans to run reasonably quickly.\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(X)\n",
    "\n",
    "# Going to implement the kmeans mode matching but with the arrays instead of as a df\n",
    "\n",
    "def kmean_confusion_mtx(kmeans=kmeans, y=y):\n",
    "    kmeans_labels = kmeans.labels_\n",
    "\n",
    "    modal_map = {}\n",
    "    for cluster_id in range(10):\n",
    "        mask = kmeans_labels == cluster_id\n",
    "        if np.sum(mask) > 0:\n",
    "            modal_map[cluster_id], _ = mode(y[mask])\n",
    "        else:\n",
    "            modal_map[cluster_id] = -1  # Handle empty clusters\n",
    "    \n",
    "    # Apply mapping\n",
    "    kmeans_labels_mapped = np.array([modal_map[label] for label in kmeans_labels])\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrix_kmeans = confusion_matrix(y, kmeans_labels_mapped)\n",
    "    print(confusion_matrix_kmeans)\n",
    "\n",
    "    total_samples = len(y)\n",
    "    correct_predictions = np.trace(confusion_matrix_kmeans)\n",
    "    confused_entries = total_samples - correct_predictions\n",
    "    \n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Confused entries: {confused_entries}\")\n",
    "    return confused_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bpow7TrZ7iB",
    "lines_to_next_cell": 2
   },
   "source": [
    "# 3. Dimensionality reduction for Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab7339",
   "metadata": {
    "id": "ejYYENCQZ9tj"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn import random_projection\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def compare_PCA_vs_Random_Proj(k=10, X = X, y = y):\n",
    "    print('fitting PCA')\n",
    "    start = time.time()\n",
    "    pca = PCA(n_components=k).fit_transform(X)\n",
    "    pca_time = time.time() - start\n",
    "    print(f'pca time: {pca_time}')\n",
    "\n",
    "    print('fitting random projection')\n",
    "    start = time.time()\n",
    "    rand = random_projection.GaussianRandomProjection(n_components=k).fit_transform(X)\n",
    "    rand_time = time.time() - start\n",
    "    print(f'rand time: {rand_time}')\n",
    "    # Compute distance correlations\n",
    "\n",
    "    print('calculating the distances')\n",
    "    orig_dist = pdist(X, 'euclidean')\n",
    "    print('orig_dist done')\n",
    "    pca_dist = pdist(pca, 'euclidean')\n",
    "    print('pca_dist_done')\n",
    "    rand_dist = pdist(rand, 'euclidean')\n",
    "    print('rand_dist done')\n",
    "    \n",
    "    pca_corr, _ = pearsonr(orig_dist, pca_dist)\n",
    "    rand_corr, _ = pearsonr(orig_dist, rand_dist)\n",
    "    \n",
    "    print(f'k={k}: PCA time={pca_time:.3f}s (corr={pca_corr:.4f}), RP time={rand_time:.3f}s (corr={rand_corr:.4f})')\n",
    "    \n",
    "    return pca_corr, rand_corr, pca_time, rand_time\n",
    "\n",
    "\n",
    "dimensions = [10, 20, 50, 100, 200]\n",
    "pca_corrs, rand_corrs = [], []\n",
    "\n",
    "# There is no way my computer can pairwise thes bad boys, especially by like this afternoon,\n",
    "# So Im goint go cut the data wayyyy down with a train/test split and only do the \"train\" data\n",
    "# Also I know there is a much better way to do all this but this needs to be done 3 days\n",
    "# ago so IDGAF\n",
    "\n",
    "X_reasonable, y_reasonable, a, b, = train_test_split(X, y, test_size = .9, random_state = 42)\n",
    "\n",
    "for k in dimensions:\n",
    "    pca_corr, rand_corr, _, _ = compare_PCA_vs_Random_Proj(k, X_reasonable, y_reasonable)\n",
    "    pca_corrs.append(pca_corr)\n",
    "    rand_corrs.append(rand_corr)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(dimensions, pca_corrs, 'o-', label='PCA')\n",
    "plt.plot(dimensions, rand_corrs, 's-', label='Random Projection')\n",
    "plt.xlabel('Target dimension k')\n",
    "plt.ylabel('Distance correlation')\n",
    "plt.legend()\n",
    "plt.savefig('latex_jail/plots/comparison_pca_vs_rand.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOTFcjWOfCZU"
   },
   "source": [
    "# 4. Clustering Fashion-MNIST using spectral clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MRB_nw21fI24",
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# The full thing wants 36.5 G of ram.... I could give it a swap file or i could do other things today\n",
    "# Since this is more then just demonstrating the lemma, Ill try it with half the data\n",
    "\n",
    "X_reasonable, y_reasonable, a, b = train_test_split(X, y, test_size = .9, random_state = 42)\n",
    "\n",
    "pca = PCA(n_components=20).fit_transform(X_reasonable)\n",
    "\n",
    "\n",
    "spectral_clustering = SpectralClustering(n_clusters = 10, assign_labels='kmeans', verbose=True).fit(pca)\n",
    "print('done with spectral clustering')\n",
    "kmean_confusion_mtx(spectral_clustering)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
