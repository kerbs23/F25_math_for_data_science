\documentclass{article}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{csvsimple}
\usepackage{pdflscape}

\usepackage{color}
\usepackage[colorlinks=true,linkcolor=red,citecolor=blue,urlcolor=cyan]{hyperref}

\definecolor{crimsonred}{RGB}{190,0,0}
\definecolor{granitepeak}{RGB}{117,142,153}
\definecolor{lakecolor}{RGB}{58,191,192}

\newcommand{\red}[1]{\textcolor{crimsonred}{#1}}
\newcommand{\DAR}[1]{\textcolor{granitepeak}{#1}}
\newcommand{\replace}[1]{\textcolor{lakecolor}{#1}}
\newcommand{\BO}[1]{\textcolor{blue}{\small {\sf BO:\@ #1}}}


\begin{document}
\noindent {\bf Bob Stutchbury} \\ 
Math 5750/6880: Mathematics of Data Science \\  
Project \#3 Final Report \\ 
\today \\


\noindent \replace{My GitHub Project3 repository is located here:
\begin{center}
\url{https://github.com/math-data-science-course/Project3}
\end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fashion-MNIST image classification using sklearn}
I went through and went through and, after establishing a base case that is very similar to the default settints, changed a bunch of the settings one at a time, looking at their scores and how long they took to converge, as well as a large one based off a preliminary run.
I gave them all 10,000 iterations to try to converge to a tolerance of .001, and let it spin for a while.
Once they finished, I tested them on the test set and got the final score, as reported on the following page.

I used this to train a final model with 6 hidden layers, of [841, 841, 841, 500, 300, 69 layers respectively.
This took about 4 minutes to train on the web machines, and resulted in a test accuraccy of .891, the best of any of the models (but not remarkably better then the other, smaller, faster iterations of the model)
It resulted in the following confusion matrix:

\csvautotabular[respect all]{/home/magnoliad/School/2025F/F25_math_for_data_science/projects/Project3/big_sklearn_cm.csv}

\begin{landscape}
\csvautotabular[respect all]{/home/magnoliad/School/2025F/F25_math_for_data_science/projects/Project3/mlp_results_20251025_170334_no_conf_mtx.csv}


\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pytorch}

Looks like I get a lot more fine-grained control of how each part of the model works.
Stack together all the different bits into a class, then run the training stuff back and forth over it in a loop.
Then, serialize the model and save it to the disk, so it can be re-loaded for testing.

It looks like there is a specific data structure that is functionally a matrix but in such a way that it can be GPUificated.
For all purposes appears to be the same as the numpy arrays we were using with sklearn.

These are managed by two things: a dataset which is defined as a class that has some sort of setup, a way to return the length of the dataset, and some way to load in one item of the data.
This is done for us with Fashion-MNIST, but they give an example of how to set up a custom one.
Then, dataloaders work over the datasets to efficently load in little random chunks for the training.

NeuralNets are built as a part of a class, with an init section where they are defined with a series of steps including a head layer and the hidden layers and a forward section where you call the init functions in the "forward" order
Then, you use the different parts of the nn package to set up phe different operations, such as layers, functions, ect
It seems like the main organizational task is understanding the different layers' functions, some paramater tuning, and making sure the size of the layers match up
Then, you set the layers up to define the forward pass by stacking up the init ed functions (methods? I dont really know python this way that well)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fashion-MNIST image classification  using PyTorch}
\DAR{Your solution goes here.}




\end{document}
