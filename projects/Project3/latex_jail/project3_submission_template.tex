

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsfonts}

\usepackage{color}
\usepackage[colorlinks=true,linkcolor=red,citecolor=blue,urlcolor=cyan]{hyperref}

\definecolor{crimsonred}{RGB}{190,0,0}
\definecolor{granitepeak}{RGB}{117,142,153}
\definecolor{lakecolor}{RGB}{58,191,192}

\newcommand{\red}[1]{\textcolor{crimsonred}{#1}}
\newcommand{\DAR}[1]{\textcolor{granitepeak}{#1}}
\newcommand{\replace}[1]{\textcolor{lakecolor}{#1}}
\newcommand{\BO}[1]{\textcolor{blue}{\small {\sf BO:\@ #1}}}


\begin{document}
\noindent {\bf Bob Stutchbury} \\ 
Math 5750/6880: Mathematics of Data Science \\  
Project \#3 Final Report \\ 
\today \\ }


\noindent \replace{My GitHub Project3 repository is located here:
\begin{center}
\url{https://github.com/math-data-science-course/Project3}
\end{center}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fashion-MNIST image classification using sklearn}
I went through and went through and, after establishing a base case that is very similar to the default settints, changed a bunch of the settings one at a time, looking at their scores and how long they took to converge, as well as a large one based off a preliminary run.
I gave them all 10,000 iterations to try to converge to a tolerance of .001, and let it spin for a while.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pytorch}

Looks like I get a lot more fine-grained control of how each part of the model works.
Stack together all the different bits into a class, then run the training stuff back and forth over it in a loop.
Then, serialize the model and save it to the disk, so it can be re-loaded for testing.

It looks like there is a specific data structure that is functionally a matrix but in such a way that it can be GPUificated.
For all purposes appears to be the same as the numpy arrays we were using with sklearn.

These are managed by two things: a dataset which is defined as a class that has some sort of setup, a way to return the length of the dataset, and some way to load in one item of the data.
This is done for us with Fashion-MNIST, but they give an example of how to set up a custom one.
Then, dataloaders work over the datasets to efficently load in little random chunks for the training.

NeuralNets are built as a part of a class, with an init section where they are defined with a series of steps including a head layer and the hidden layers and a forward section where you call the init functions in the "forward" order
Then, you use the different parts of the nn package to set up phe different operations, such as layers, functions, ect






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fashion-MNIST image classification  using PyTorch}
\DAR{Your solution goes here.}




\end{document}
